from DrissionPage import ChromiumPage, ChromiumOptions
import csv
import time
import os
import random
import json

def crawl_dcard_passive_content():
    # --- 1. è¨­å®šç€è¦½å™¨ ---
    co = ChromiumOptions()
    possible_paths = [
        r"C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe",
        r"C:\Program Files\Microsoft\Edge\Application\msedge.exe"
    ]
    edge_path = next((p for p in possible_paths if os.path.exists(p)), None)
    if edge_path: co.set_browser_path(edge_path)
    try: co.use_system_user_path()
    except: pass
    co.set_local_port(9527)

    print("ğŸš€ å•Ÿå‹•ç€è¦½å™¨ (å…¨è¢«å‹•ç›£è½æ¨¡å¼)...")
    try:
        page = ChromiumPage(co)
    except Exception as e:
        print(f"âŒ å•Ÿå‹•å¤±æ•—: {e}")
        return []

    # ==========================================
    # Phase 1: æœå°‹åˆ—è¡¨ (å·²é©—è­‰æˆåŠŸ)
    # ==========================================
    print("running Phase 1...")
    page.listen.start('search/all')
    
    target_url = "https://www.dcard.tw/search?query=å°ç©é›»&forum=tech_job"
    if target_url not in page.url:
        page.get(target_url)

    # æ²å‹•å¹¾æ¬¡æŠ“åˆ—è¡¨
    scroll_times = 3
    for i in range(scroll_times):
        page.scroll.to_bottom()
        time.sleep(2)

    all_posts = []
    seen_ids = set()

    # è§£æåˆ—è¡¨
    for packet in page.listen.steps(timeout=5):
        try:
            response = packet.response.body
            items = []
            if isinstance(response, dict):
                items = response.get('items', []) or response.get('data', [])
            elif isinstance(response, list):
                items = response
            
            for item in items:
                try:
                    search_post = item.get('searchPost', {})
                    real_post = search_post.get('post', {})
                    if not real_post: continue

                    post_id = str(real_post.get('id'))
                    title = real_post.get('title', '').replace('<em>', '').replace('</em>', '')
                    created_at = real_post.get('createdAt')
                    post_url = f"https://www.dcard.tw/f/tech_job/p/{post_id}"

                    if post_id in seen_ids: continue
                    seen_ids.add(post_id)
                    
                    all_posts.append({
                        "ID": post_id,
                        "æ¨™é¡Œ": title,
                        "é€£çµ": post_url,
                        "ç™¼æ–‡æ™‚é–“": created_at,
                        "å…¨æ–‡": "" # å¾…å¡«å…¥
                    })
                except: continue
        except: continue

    print(f"âœ… Phase 1 å®Œæˆï¼å…±å–å¾— {len(all_posts)} ç¯‡ IDã€‚")
    
    # åœæ­¢ä¹‹å‰çš„ç›£è½
    page.listen.stop()

    # ==========================================
    # Phase 2: é€²å…¥æ–‡ç« é é¢ç›£è½å…§æ–‡ (æ ¸å¿ƒä¿®æ­£)
    # ==========================================
    print(f"ğŸš€ [Phase 2] é–‹å§‹é€²å…¥æ–‡ç« é é¢ç›£è½å…¨æ–‡...")
    
    success_count = 0
    
    for index, post in enumerate(all_posts):
        post_id = post['ID']
        post_url = post['é€£çµ']
        
        # 1. è¨­å®šç›£è½å™¨ï¼šé–å®š "posts/{id}" é€™å€‹ API
        # ç•¶æˆ‘å€‘æ‰“é–‹ç¶²é æ™‚ï¼Œç€è¦½å™¨æœƒè‡ªå‹•æ‰“é€™å€‹ APIï¼Œæˆ‘å€‘åªè¦æ””æˆªå®ƒ
        listen_target = f'posts/{post_id}'
        page.listen.start(listen_target)
        
        print(f"   ğŸ“– ({index+1}/{len(all_posts)}) æ­£åœ¨é–±è®€: {post['æ¨™é¡Œ'][:15]}...", end="\r")
        
        try:
            # 2. ã€é—œéµã€‘æ‰“é–‹ã€Œç¶²é ã€è€Œä¸æ˜¯ API
            page.get(post_url)
            
            # 3. ç­‰å¾…æ””æˆªå°åŒ…
            found_content = False
            
            # çµ¦å®ƒ 5 ç§’é˜è¼‰å…¥ä¸¦è§¸ç™¼ API
            for packet in page.listen.steps(timeout=5):
                try:
                    res = packet.response.body
                    # ç¢ºèªæ˜¯ä¸æ˜¯é€™ç¯‡æ–‡ç« çš„è³‡æ–™
                    if isinstance(res, dict) and str(res.get('id')) == post_id:
                        content = res.get('content', '')
                        if content:
                            # ç°¡å–®æ¸…ç†æ›è¡Œ
                            post['å…¨æ–‡'] = content.replace('\n', ' ')
                            found_content = True
                            success_count += 1
                            break
                except: continue
            
            if not found_content:
                # å¦‚æœç›£è½å¤±æ•—ï¼Œå˜—è©¦ç”¨å‚™ç”¨æ–¹æ¡ˆï¼šç›´æ¥æŠ“ç¶²é æ–‡å­— (DOM)
                # print("      (ç›£è½é€¾æ™‚ï¼Œæ”¹ç”¨ç¶²é æ–‡å­—æŠ“å–)")
                article = page.ele('tag:article')
                if article:
                    post['å…¨æ–‡'] = article.text.replace('\n', ' ')
                    success_count += 1
                else:
                    post['å…¨æ–‡'] = "(ç„¡æ³•è®€å–å…§å®¹)"

            # 4. ä¼‘æ¯ä¸€ä¸‹
            page.listen.stop()
            time.sleep(random.uniform(1.5, 2.5))

        except Exception as e:
            post['å…¨æ–‡'] = "è®€å–éŒ¯èª¤"
            page.listen.stop()
            continue

    print(f"\n\nğŸ‰ å…¨éƒ¨å®Œæˆï¼æˆåŠŸä¸‹è¼‰ {success_count}/{len(all_posts)} ç¯‡å…¨æ–‡ã€‚")
    return all_posts

# --- åŸ·è¡Œ ---
data = crawl_dcard_passive_content()

if data:
    filename = 'dcard_tsmc_passive_full.csv'
    with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=data[0].keys())
        writer.writeheader()
        writer.writerows(data)
    print(f"ğŸ“‚ æª”æ¡ˆå·²å„²å­˜: {filename}")
else:
    print("ğŸ˜­ æ²’æŠ“åˆ°è³‡æ–™")